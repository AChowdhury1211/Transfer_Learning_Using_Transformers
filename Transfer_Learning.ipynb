{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ray\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append(\"..\")\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "from dotenv import load_dotenv; load_dotenv()\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "if ray.is_initialized():\n",
    "    ray.shutdown()\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.cluster_resources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EFS_DIR = f\"/efs/shared_storage/madewithml/{os.environ['GITHUB_USERNAME']}\"\n",
    "print (EFS_DIR)num_workers = 1\n",
    "resources_per_worker={\"CPU\": 3, \"GPU\": 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data ingestion\n",
    "DATASET_LOC = \"https://github.com/AChowdhury1211/mlops_project/mlops/datasets/dataset.csv\"\n",
    "df = pd.read_csv(DATASET_LOC)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tag.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "train_df, val_df = train_test_split(df, stratify=df.tag, test_size=test_size, random_state=1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.tag.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df.tag.value_counts() * int((1-test_size) / test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set_theme()\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "from wordcloud import WordCloud, STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tags = Counter(df.tag)\n",
    "all_tags.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags, tag_counts = zip(*all_tags.most_common())\n",
    "plt.figure(figsize=(10, 3))\n",
    "ax = sns.barplot(x=list(tags), y=list(tag_counts))\n",
    "ax.set_xticklabels(tags, rotation=0, fontsize=12)\n",
    "plt.title(\"Tag distribution\", fontsize=16)\n",
    "plt.ylabel(\"# of projects\", fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag=\"natural-language-processing\"\n",
    "plt.figure(figsize=(10, 3))\n",
    "subset = df[df.tag==tag]\n",
    "text = subset.title.values\n",
    "cloud = WordCloud(\n",
    "    stopwords=STOPWORDS, background_color=\"black\", collocations=False,\n",
    "    width=500, height=300).generate(\" \".join(text))\n",
    "plt.axis(\"off\")\n",
    "plt.imshow(cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = df.title + \" \" + df.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"stopwords\")\n",
    "STOPWORDS = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, stopwords=STOPWORDS):\n",
    "    text = text.lower()\n",
    "\n",
    "    pattern = re.compile(r'\\b(' + r\"|\".join(stopwords) + r\")\\b\\s*\")\n",
    "    text = pattern.sub('', text)\n",
    "\n",
    "    text = re.sub(r\"([!\\\"'#$%&()*\\+,-./:;<=>?@\\\\\\[\\]^_`{|}~])\", r\" \\1 \", text) \n",
    "    text = re.sub(\"[^A-Za-z0-9]+\", \" \", text)  \n",
    "    text = re.sub(\" +\", \" \", text)  \n",
    "    text = text.strip()  \n",
    "    text = re.sub(r\"http\\S+\", \"\", text)  \n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_df = df.copy()\n",
    "df.text = df.text.apply(clean_text)\n",
    "print (f\"{original_df.text.values[0]}\\n{df.text.values[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = df.drop(columns=[\"id\", \"created_on\", \"title\", \"description\"], errors=\"ignore\") \n",
    "df = df[[\"text\", \"tag\"]] \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = train_df.tag.unique().tolist()\n",
    "num_classes = len(tags)\n",
    "class_to_index = {tag: i for i, tag in enumerate(tags)}\n",
    "class_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"tag\"] = df[\"tag\"].map(class_to_index)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(indices, index_to_class):\n",
    "    return [index_to_class[index] for index in indices]\n",
    "index_to_class = {v:k for k, v in class_to_index.items()}\n",
    "decode(df.head()[\"tag\"].values, index_to_class=index_to_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bert tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\", return_dict=False)\n",
    "text = \"Transfer learning with transformers for text classification.\"\n",
    "encoded_inputs = tokenizer([text], return_tensors=\"np\", padding=\"longest\")  \n",
    "print (\"input_ids:\", encoded_inputs[\"input_ids\"])\n",
    "print (\"attention_mask:\", encoded_inputs[\"attention_mask\"])\n",
    "print (tokenizer.decode(encoded_inputs[\"input_ids\"][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(batch):\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"allenai/scibert_scivocab_uncased\", return_dict=False)\n",
    "    encoded_inputs = tokenizer(batch[\"text\"].tolist(), return_tensors=\"np\", padding=\"longest\")\n",
    "    return dict(ids=encoded_inputs[\"input_ids\"], masks=encoded_inputs[\"attention_mask\"], targets=np.array(batch[\"tag\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize(df.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(df, class_to_index):\n",
    "    df[\"text\"] = df.title + \" \" + df.description  \n",
    "    df[\"text\"] = df.text.apply(clean_text)  \n",
    "    df = df.drop(columns=[\"id\", \"created_on\", \"title\", \"description\"], errors=\"ignore\")  \n",
    "    df = df[[\"text\", \"tag\"]]  \n",
    "    df[\"tag\"] = df[\"tag\"].map(class_to_index)  \n",
    "    outputs = tokenize(df)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess(df=train_df, class_to_index=class_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distributed Preprocessing\n",
    "from src.data import stratify_split\n",
    "ray.data.DatasetContext.get_current().execution_options.preserve_order = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = ray.data.read_csv(DATASET_LOC)\n",
    "ds = ds.random_shuffle(seed=1234)\n",
    "ds.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = 0.2\n",
    "train_ds, val_ds = stratify_split(ds, stratify=\"tag\", test_size=test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags = train_ds.unique(column=\"tag\")\n",
    "class_to_index = {tag: i for i, tag in enumerate(tags)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ds = train_ds.map_batches(preprocess, fn_kwargs={\"class_to_index\": class_to_index}, batch_format=\"pandas\")\n",
    "sample_ds.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "from ray.data.preprocessor import Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seeds(seed=42):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    eval(\"setattr(torch.backends.cudnn, 'deterministic', True)\")\n",
    "    eval(\"setattr(torch.backends.cudnn, 'benchmark', False)\")\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(num_samples=None):\n",
    "    ds = ray.data.read_csv(DATASET_LOC)\n",
    "    ds = ds.random_shuffle(seed=1234)\n",
    "    ds = ray.data.from_items(ds.take(num_samples)) if num_samples else ds\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomPreprocessor():\n",
    "    def __init__(self, class_to_index={}):\n",
    "        self.class_to_index = class_to_index or {}  \n",
    "        self.index_to_class = {v: k for k, v in self.class_to_index.items()}\n",
    "        \n",
    "    def fit(self, ds):\n",
    "        tags = ds.unique(column=\"tag\")\n",
    "        self.class_to_index = {tag: i for i, tag in enumerate(tags)}\n",
    "        self.index_to_class = {v:k for k, v in self.class_to_index.items()}\n",
    "        return self\n",
    "    \n",
    "    def transform(self, ds):\n",
    "        return ds.map_batches(\n",
    "            preprocess, \n",
    "            fn_kwargs={\"class_to_index\": self.class_to_index}, \n",
    "            batch_format=\"pandas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "from transformers import BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = BertModel.from_pretrained(\"allenai/scibert_scivocab_uncased\", return_dict=False)\n",
    "embedding_dim = llm.config.hidden_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Transfer learning with transformers for text classification.\"\n",
    "batch = tokenizer([text], return_tensors=\"np\", padding=\"longest\")\n",
    "batch = {k:torch.tensor(v) for k,v in batch.items()}  \n",
    "seq, pool = llm(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"])\n",
    "np.shape(seq), np.shape(pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FinetunedLLM(nn.Module):\n",
    "    def __init__(self, llm, dropout_p, embedding_dim, num_classes):\n",
    "        super(FinetunedLLM, self).__init__()\n",
    "        self.llm = llm\n",
    "        self.dropout_p = dropout_p\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_classes = num_classes\n",
    "        self.dropout = torch.nn.Dropout(dropout_p)\n",
    "        self.fc1 = torch.nn.Linear(embedding_dim, num_classes)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        ids, masks = batch[\"ids\"], batch[\"masks\"]\n",
    "        seq, pool = self.llm(input_ids=ids, attention_mask=masks)\n",
    "        z = self.dropout(pool)\n",
    "        z = self.fc1(z)\n",
    "        return z\n",
    "    \n",
    "    @torch.inference_mode()\n",
    "    def predict(self, batch):\n",
    "        self.eval()\n",
    "        z = self(batch)\n",
    "        y_pred = torch.argmax(z, dim=1).cpu().numpy()\n",
    "        return y_pred\n",
    "    \n",
    "    @torch.inference_mode()\n",
    "    def predict_proba(self, batch):\n",
    "        self.eval()\n",
    "        z = self(batch)\n",
    "        y_probs = F.softmax(z, dim=1).cpu().numpy()\n",
    "        return y_probs\n",
    "    \n",
    "    def save(self, dp):\n",
    "        with open(Path(dp, \"args.json\"), \"w\") as fp:\n",
    "            contents = {\n",
    "                \"dropout_p\": self.dropout_p,\n",
    "                \"embedding_dim\": self.embedding_dim,\n",
    "                \"num_classes\": self.num_classes,\n",
    "            }\n",
    "            json.dump(contents, fp, indent=4, sort_keys=False)\n",
    "        torch.save(self.state_dict(), os.path.join(dp, \"model.pt\"))\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, args_fp, state_dict_fp):\n",
    "        with open(args_fp, \"r\") as fp:\n",
    "            kwargs = json.load(fp=fp)\n",
    "        llm = BertModel.from_pretrained(\"allenai/scibert_scivocab_uncased\", return_dict=False)\n",
    "        model = cls(llm=llm, **kwargs)\n",
    "        model.load_state_dict(torch.load(state_dict_fp, map_location=torch.device(\"cpu\")))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FinetunedLLM(llm=llm, dropout_p=0.5, embedding_dim=embedding_dim, num_classes=num_classes)\n",
    "print (model.named_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train.torch import get_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_array(arr, dtype=np.int32):\n",
    "    max_len = max(len(row) for row in arr)\n",
    "    padded_arr = np.zeros((arr.shape[0], max_len), dtype=dtype)\n",
    "    for i, row in enumerate(arr):\n",
    "        padded_arr[i][:len(row)] = row\n",
    "    return padded_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    batch[\"ids\"] = pad_array(batch[\"ids\"])\n",
    "    batch[\"masks\"] = pad_array(batch[\"masks\"])\n",
    "    dtypes = {\"ids\": torch.int32, \"masks\": torch.int32, \"targets\": torch.int64}\n",
    "    tensor_batch = {}\n",
    "    for key, array in batch.items():\n",
    "        tensor_batch[key] = torch.as_tensor(array, dtype=dtypes[key], device=get_device())\n",
    "    return tensor_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_batch = sample_ds.take_batch(batch_size=128)\n",
    "collate_fn(batch=sample_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import ray.train as train\n",
    "from ray.train import Checkpoint, CheckpointConfig, DataConfig, RunConfig, ScalingConfig\n",
    "from ray.train.torch import TorchCheckpoint, TorchTrainer\n",
    "import tempfile\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.parallel.distributed import DistributedDataParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(ds, batch_size, model, num_classes, loss_fn, optimizer):\n",
    "    model.train()\n",
    "    loss = 0.0\n",
    "    ds_generator = ds.iter_torch_batches(batch_size=batch_size, collate_fn=collate_fn)\n",
    "    for i, batch in enumerate(ds_generator):\n",
    "        optimizer.zero_grad()  \n",
    "        z = model(batch) \n",
    "        targets = F.one_hot(batch[\"targets\"], num_classes=num_classes).float()  \n",
    "        J = loss_fn(z, targets) \n",
    "        J.backward()  \n",
    "        optimizer.step()  \n",
    "        loss += (J.detach().item() - loss) / (i + 1)  \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_step(ds, batch_size, model, num_classes, loss_fn):\n",
    "    model.eval()\n",
    "    loss = 0.0\n",
    "    y_trues, y_preds = [], []\n",
    "    ds_generator = ds.iter_torch_batches(batch_size=batch_size, collate_fn=collate_fn)\n",
    "    with torch.inference_mode():\n",
    "        for i, batch in enumerate(ds_generator):\n",
    "            z = model(batch)\n",
    "            targets = F.one_hot(batch[\"targets\"], num_classes=num_classes).float()  \n",
    "            J = loss_fn(z, targets).item()\n",
    "            loss += (J - loss) / (i + 1)\n",
    "            y_trues.extend(batch[\"targets\"].cpu().numpy())\n",
    "            y_preds.extend(torch.argmax(z, dim=1).cpu().numpy())\n",
    "    return loss, np.vstack(y_trues), np.vstack(y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_loop_per_worker(config):\n",
    "    # Hyperparameters\n",
    "    dropout_p = config[\"dropout_p\"]\n",
    "    lr = config[\"lr\"]\n",
    "    lr_factor = config[\"lr_factor\"]\n",
    "    lr_patience = config[\"lr_patience\"]\n",
    "    num_epochs = config[\"num_epochs\"]\n",
    "    batch_size = config[\"batch_size\"]\n",
    "    num_classes = config[\"num_classes\"]\n",
    "\n",
    "    set_seeds()\n",
    "    train_ds = train.get_dataset_shard(\"train\")\n",
    "    val_ds = train.get_dataset_shard(\"val\")\n",
    "\n",
    "    # Model\n",
    "    llm = BertModel.from_pretrained(\"allenai/scibert_scivocab_uncased\", return_dict=False)\n",
    "    model = FinetunedLLM(llm=llm, dropout_p=dropout_p, embedding_dim=llm.config.hidden_size, num_classes=num_classes)\n",
    "    model = train.torch.prepare_model(model)\n",
    "\n",
    "\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=lr_factor, patience=lr_patience)\n",
    "\n",
    "    # Training\n",
    "    num_workers = train.get_context().get_world_size()\n",
    "    batch_size_per_worker = batch_size // num_workers\n",
    "    for epoch in range(num_epochs):\n",
    "\n",
    "        train_loss = train_step(train_ds, batch_size_per_worker, model, num_classes, loss_fn, optimizer)\n",
    "        val_loss, _, _ = eval_step(val_ds, batch_size_per_worker, model, num_classes, loss_fn)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "\n",
    "        with tempfile.TemporaryDirectory() as dp:\n",
    "            if isinstance(model, DistributedDataParallel): \n",
    "                model.module.save(dp=dp)\n",
    "            else:\n",
    "                model.save(dp=dp)\n",
    "            metrics = dict(epoch=epoch, lr=optimizer.param_groups[0][\"lr\"], train_loss=train_loss, val_loss=val_loss)\n",
    "            checkpoint = Checkpoint.from_directory(dp)\n",
    "            train.report(metrics, checkpoint=checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Configurations\n",
    "from src.config import EFS_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loop_config = {\n",
    "    \"dropout_p\": 0.5,\n",
    "    \"lr\": 1e-4,\n",
    "    \"lr_factor\": 0.8,\n",
    "    \"lr_patience\": 3,\n",
    "    \"num_epochs\": 10,\n",
    "    \"batch_size\": 256,\n",
    "    \"num_classes\": num_classes,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaling_config = ScalingConfig(\n",
    "    num_workers=num_workers,\n",
    "    use_gpu=bool(resources_per_worker[\"GPU\"]),\n",
    "    resources_per_worker=resources_per_worker\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_config = CheckpointConfig(num_to_keep=1, checkpoint_score_attribute=\"val_loss\", checkpoint_score_order=\"min\")\n",
    "run_config = RunConfig(name=\"llm\", checkpoint_config=checkpoint_config, storage_path=EFS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_data()\n",
    "train_ds, val_ds = stratify_split(ds, stratify=\"tag\", test_size=test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "options = ray.data.ExecutionOptions(preserve_order=True)\n",
    "dataset_config = DataConfig(\n",
    "    datasets_to_split=[\"train\"],\n",
    "    execution_options=options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker=train_loop_per_worker,\n",
    "    train_loop_config=train_loop_config,\n",
    "    scaling_config=scaling_config,\n",
    "    run_config=run_config,\n",
    "    datasets={\"train\": train_ds, \"val\": val_ds},\n",
    "    dataset_config=dataset_config,\n",
    "    metadata={\"class_to_index\": preprocessor.class_to_index}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "results = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.metrics_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.best_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TorchPredictor:\n",
    "    def __init__(self, preprocessor, model):\n",
    "        self.preprocessor = preprocessor\n",
    "        self.model = model\n",
    "        self.model.eval()\n",
    "        \n",
    "    def __call__(self, batch):\n",
    "        results = self.model.predict(collate_fn(batch))\n",
    "        return {\"output\": results}\n",
    "\n",
    "    def predict_proba(self, batch):\n",
    "        results = self.model.predict_proba(collate_fn(batch))\n",
    "        return {\"output\": results}\n",
    "        \n",
    "    def get_preprocessor(self):\n",
    "        return self.preprocessor\n",
    "        \n",
    "    @classmethod\n",
    "    def from_checkpoint(cls, checkpoint):\n",
    "        metadata = checkpoint.get_metadata()\n",
    "        preprocessor = CustomPreprocessor(class_to_index=metadata[\"class_to_index\"])\n",
    "        model = FinetunedLLM.load(Path(checkpoint.path, \"args.json\"), Path(checkpoint.path, \"model.pt\"))\n",
    "        return cls(preprocessor=preprocessor, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Artifacts\n",
    "best_checkpoint = results.best_checkpoints[0][0]\n",
    "predictor = TorchPredictor.from_checkpoint(best_checkpoint)\n",
    "preprocessor = predictor.get_preprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOLDOUT_LOC = \"https://github.com/AChowdhury1211/mlops_project/mlops/datasets/holdout.csv\"\n",
    "test_ds = ray.data.read_csv(HOLDOUT_LOC)\n",
    "preprocessed_ds = preprocessor.transform(test_ds)\n",
    "preprocessed_ds.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = preprocessed_ds.select_columns(cols=[\"targets\"]).take_all()\n",
    "y_true = np.stack([item[\"targets\"] for item in values])\n",
    "print (y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = preprocessed_ds.map_batches(predictor).take_all()\n",
    "y_pred = np.array([d[\"output\"] for d in predictions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
    "{\"precision\": metrics[0], \"recall\": metrics[1], \"f1\": metrics[2]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(ds, predictor):\n",
    "    preprocessor = predictor.get_preprocessor()\n",
    "    preprocessed_ds = preprocessor.transform(ds)\n",
    "    values = preprocessed_ds.select_columns(cols=[\"targets\"]).take_all()\n",
    "    y_true = np.stack([item[\"targets\"] for item in values])\n",
    "    \n",
    "    predictions = preprocessed_ds.map_batches(predictor).take_all()\n",
    "    y_pred = np.array([d[\"output\"] for d in predictions])\n",
    "\n",
    "    metrics = precision_recall_fscore_support(y_true, y_pred, average=\"weighted\")\n",
    "    performance = {\"precision\": metrics[0], \"recall\": metrics[1], \"f1\": metrics[2]}\n",
    "    return performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = evaluate(ds=test_ds, predictor=predictor)\n",
    "print (json.dumps(performance, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_prob(prob, index_to_class):\n",
    "    d = {}\n",
    "    for i, item in enumerate(prob):\n",
    "        d[index_to_class[i]] = item\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_proba(ds, predictor):\n",
    "    preprocessor = predictor.get_preprocessor()\n",
    "    preprocessed_ds = preprocessor.transform(ds)\n",
    "    outputs = preprocessed_ds.map_batches(predictor.predict_proba)\n",
    "    y_prob = np.array([d[\"output\"] for d in outputs.take_all()])\n",
    "    results = []\n",
    "    for i, prob in enumerate(y_prob):\n",
    "        tag = preprocessor.index_to_class[prob.argmax()]\n",
    "        results.append({\"prediction\": tag, \"probabilities\": format_prob(prob, preprocessor.index_to_class)})\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Transfer learning with transformers\"\n",
    "description = \"Using transformers for transfer learning on text classification tasks.\"\n",
    "sample_ds = ray.data.from_items([{\"title\": title, \"description\": description, \"tag\": \"other\"}])\n",
    "predict_proba(ds=sample_ds, predictor=predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MLflow\n",
    "import mlflow\n",
    "from pathlib import Path\n",
    "from ray.tune.logger.mlflow import MLflowLoggerCallback\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_REGISTRY = Path(f\"{EFS_DIR}/mlflow\")\n",
    "Path(MODEL_REGISTRY).mkdir(parents=True, exist_ok=True)\n",
    "MLFLOW_TRACKING_URI = \"file://\" + str(MODEL_REGISTRY.absolute())\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "print (mlflow.get_tracking_uri())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment_name = f\"llm-{int(time.time())}\"\n",
    "mlflow_callback = MLflowLoggerCallback(\n",
    "    tracking_uri=MLFLOW_TRACKING_URI,\n",
    "    experiment_name=experiment_name,\n",
    "    save_artifact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_config = RunConfig(\n",
    "    callbacks=[mlflow_callback],\n",
    "    checkpoint_config=checkpoint_config,\n",
    "    storage_path=EFS_DIR,\n",
    "    local_dir=EFS_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_data()\n",
    "train_ds, val_ds = stratify_split(ds, stratify=\"tag\", test_size=test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = CustomPreprocessor()\n",
    "preprocessor = preprocessor.fit(train_ds)\n",
    "train_ds = preprocessor.transform(train_ds)\n",
    "val_ds = preprocessor.transform(val_ds)\n",
    "train_ds = train_ds.materialize()\n",
    "val_ds = val_ds.materialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker=train_loop_per_worker,\n",
    "    train_loop_config=train_loop_config,\n",
    "    scaling_config=scaling_config,\n",
    "    run_config=run_config,  \n",
    "    datasets={\"train\": train_ds, \"val\": val_ds},\n",
    "    dataset_config=dataset_config,\n",
    "    metadata={\"class_to_index\": preprocessor.class_to_index}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "results = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.metrics_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_runs = mlflow.search_runs(experiment_names=[experiment_name], order_by=[\"metrics.val_loss ASC\"])\n",
    "sorted_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_run = sorted_runs.iloc[0]\n",
    "best_run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.train import Result\n",
    "from urllib.parse import urlparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_checkpoint(run_id):\n",
    "    artifact_dir = urlparse(mlflow.get_run(run_id).info.artifact_uri).path \n",
    "    results = Result.from_path(artifact_dir)\n",
    "    return results.best_checkpoints[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_checkpoint = get_best_checkpoint(run_id=best_run.run_id)\n",
    "predictor = TorchPredictor.from_checkpoint(best_checkpoint)\n",
    "preprocessor = predictor.get_preprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = evaluate(ds=test_ds, predictor=predictor)\n",
    "print (json.dumps(performance, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Transfer learning with transformers\"\n",
    "description = \"Using transformers for transfer learning on text classification tasks.\"\n",
    "sample_ds = ray.data.from_items([{\"title\": title, \"description\": description, \"tag\": \"other\"}])\n",
    "predict_proba(ds=sample_ds, predictor=predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter Tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "from ray.tune import Tuner\n",
    "from ray.tune.schedulers import AsyncHyperBandScheduler\n",
    "from ray.tune.search import ConcurrencyLimiter\n",
    "from ray.tune.search.hyperopt import HyperOptSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_runs = 2\n",
    "set_seeds()\n",
    "ds = load_data()\n",
    "train_ds, val_ds = stratify_split(ds, stratify=\"tag\", test_size=test_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = CustomPreprocessor()\n",
    "preprocessor = preprocessor.fit(train_ds)\n",
    "train_ds = preprocessor.transform(train_ds)\n",
    "val_ds = preprocessor.transform(val_ds)\n",
    "train_ds = train_ds.materialize()\n",
    "val_ds = val_ds.materialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = TorchTrainer(\n",
    "    train_loop_per_worker=train_loop_per_worker,\n",
    "    train_loop_config=train_loop_config,\n",
    "    scaling_config=scaling_config,\n",
    "    datasets={\"train\": train_ds, \"val\": val_ds},\n",
    "    dataset_config=dataset_config,\n",
    "    metadata={\"class_to_index\": preprocessor.class_to_index}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mlflow_callback = MLflowLoggerCallback(\n",
    "    tracking_uri=MLFLOW_TRACKING_URI,\n",
    "    experiment_name=experiment_name,\n",
    "    save_artifact=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_config = CheckpointConfig(num_to_keep=1, checkpoint_score_attribute=\"val_loss\", checkpoint_score_order=\"min\")\n",
    "run_config = RunConfig(\n",
    "    callbacks=[mlflow_callback],\n",
    "    checkpoint_config=checkpoint_config,\n",
    "    storage_path=EFS_DIR,\n",
    "    local_dir=EFS_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_params = [{\"train_loop_config\": {\"dropout_p\": 0.5, \"lr\": 1e-4, \"lr_factor\": 0.8, \"lr_patience\": 3}}]\n",
    "search_alg = HyperOptSearch(points_to_evaluate=initial_params)\n",
    "search_alg = ConcurrencyLimiter(search_alg, max_concurrent=2)  # trade off b/w optimization and search space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_space = {\n",
    "    \"train_loop_config\": {\n",
    "        \"dropout_p\": tune.uniform(0.3, 0.9),\n",
    "        \"lr\": tune.loguniform(1e-5, 5e-4),\n",
    "        \"lr_factor\": tune.uniform(0.1, 0.9),\n",
    "        \"lr_patience\": tune.uniform(1, 10),\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = AsyncHyperBandScheduler(\n",
    "    max_t=train_loop_config[\"num_epochs\"], \n",
    "    grace_period=5,  \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_config = tune.TuneConfig(\n",
    "    metric=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    search_alg=search_alg,\n",
    "    scheduler=scheduler,\n",
    "    num_samples=num_runs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = Tuner(\n",
    "    trainable=trainer,\n",
    "    run_config=run_config,\n",
    "    param_space=param_space,\n",
    "    tune_config=tune_config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "results = tuner.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.get_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial = results.get_best_result(metric=\"val_loss\", mode=\"min\")\n",
    "best_trial.metrics_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial.config[\"train_loop_config\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_runs = mlflow.search_runs(experiment_names=[experiment_name], order_by=[\"metrics.val_loss ASC\"])\n",
    "sorted_runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_checkpoint = get_best_checkpoint(run_id=best_run.run_id)\n",
    "predictor = TorchPredictor.from_checkpoint(best_checkpoint)\n",
    "preprocessor = predictor.get_preprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "performance = evaluate(ds=test_ds, predictor=predictor)\n",
    "print (json.dumps(performance, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Transfer learning with transformers\"\n",
    "description = \"Using transformers for transfer learning on text classification tasks.\"\n",
    "sample_ds = ray.data.from_items([{\"title\": title, \"description\": description, \"tag\": \"other\"}])\n",
    "predict_proba(ds=sample_ds, predictor=predictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluation\n",
    "metrics = {\"overall\": {}, \"class\": {}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = TorchPredictor.from_checkpoint(best_checkpoint)\n",
    "preprocessor = predictor.get_preprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_ds = preprocessor.transform(test_ds)\n",
    "values = preprocessed_ds.select_columns(cols=[\"targets\"]).take_all()\n",
    "y_test = np.stack([item[\"targets\"] for item in values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = preprocessed_ds.map_batches(predictor.predict_proba)\n",
    "y_prob = np.array([d[\"output\"] for d in outputs.take_all()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (np.shape(y_test))\n",
    "print (np.shape(y_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = test_ds.to_pandas()\n",
    "test_df[\"text\"] = test_df[\"title\"] + \" \" + test_df[\"description\"]\n",
    "test_df[\"prediction\"] = test_df.index.map(lambda i: preprocessor.index_to_class[y_pred[i]])\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_metrics = precision_recall_fscore_support(y_test, y_pred, average=\"weighted\")\n",
    "metrics[\"overall\"][\"precision\"] = overall_metrics[0]\n",
    "metrics[\"overall\"][\"recall\"] = overall_metrics[1]\n",
    "metrics[\"overall\"][\"f1\"] = overall_metrics[2]\n",
    "metrics[\"overall\"][\"num_samples\"] = np.float64(len(y_test))\n",
    "print (json.dumps(metrics[\"overall\"], indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_metrics = precision_recall_fscore_support(y_test, y_pred, average=None)\n",
    "for i, _class in enumerate(preprocessor.class_to_index):\n",
    "    metrics[\"class\"][_class] = {\n",
    "        \"precision\": class_metrics[0][i],\n",
    "        \"recall\": class_metrics[1][i],\n",
    "        \"f1\": class_metrics[2][i],\n",
    "        \"num_samples\": np.float64(class_metrics[3][i]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = \"natural-language-processing\"\n",
    "print (json.dumps(metrics[\"class\"][tag], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_tags_by_f1 = OrderedDict(sorted(\n",
    "        metrics[\"class\"].items(), key=lambda tag: tag[1][\"f1\"], reverse=True))\n",
    "for item in sorted_tags_by_f1.items():\n",
    "    print (json.dumps(item, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = \"natural-language-processing\"\n",
    "index = preprocessor.class_to_index[tag]\n",
    "tp, fp, fn = [], [], []\n",
    "for i, true in enumerate(y_test):\n",
    "    pred = y_pred[i]\n",
    "    if index==true==pred:\n",
    "        tp.append(i)\n",
    "    elif index!=true and index==pred:\n",
    "        fp.append(i)\n",
    "    elif index==true and index!=pred:\n",
    "        fn.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (tp)\n",
    "print (fp)\n",
    "print (fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 3\n",
    "cm = [(tp, \"True positives\"), (fp, \"False positives\"), (fn, \"False negatives\")]\n",
    "for item in cm:\n",
    "    if len(item[0]):\n",
    "        print (f\"\\n=== {item[1]} ===\")\n",
    "        for index in item[0][:num_samples]:\n",
    "            print (f\"{test_df.iloc[index].text}\")\n",
    "            print (f\"    true: {test_df.tag[index]}\")\n",
    "            print (f\"    pred: {test_df.prediction[index]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COnfidence Learning\n",
    "tag = \"natural-language-processing\"\n",
    "index = class_to_index[tag]\n",
    "indices = np.where(y_test==index)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_confidence = []\n",
    "min_threshold = 0.5\n",
    "for i in indices:\n",
    "    prob = y_prob[i][index]\n",
    "    if prob <= 0.5:\n",
    "        low_confidence.append({\n",
    "            \"text\": f\"{test_df.iloc[i].text}\",\n",
    "            \"true\": test_df.tag[i], \n",
    "            \"pred\": test_df.prediction[i], \n",
    "            \"prob\": prob})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_confidence[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cleanlab\n",
    "from cleanlab.filter import find_label_issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_issues = find_label_issues(labels=y_test, pred_probs=y_prob, return_indices_ranked_by=\"self_confidence\")\n",
    "test_df.iloc[label_issues].drop(columns=[\"text\"]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Slicing \n",
    "from snorkel.slicing import PandasSFApplier\n",
    "from snorkel.slicing import slice_dataframe\n",
    "from snorkel.slicing import slicing_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@slicing_function()\n",
    "def nlp_llm(x):\n",
    "    nlp_project = \"natural-language-processing\" in x.tag\n",
    "    llm_terms = [\"transformer\", \"llm\", \"bert\"]\n",
    "    llm_project = any(s.lower() in x.text.lower() for s in llm_terms)\n",
    "    return (nlp_project and llm_project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@slicing_function()\n",
    "def short_text(x):\n",
    "    return len(x.text.split()) < 8  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_llm_df = slice_dataframe(test_df, nlp_llm)\n",
    "nlp_llm_df[[\"text\", \"tag\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_text_df = slice_dataframe(test_df, short_text)\n",
    "short_text_df[[\"text\", \"tag\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slicing_functions = [nlp_llm, short_text]\n",
    "applier = PandasSFApplier(slicing_functions)\n",
    "slices = applier.apply(test_df)\n",
    "slices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics[\"slices\"] = {}\n",
    "for slice_name in slices.dtype.names:\n",
    "    mask = slices[slice_name].astype(bool)\n",
    "    if sum(mask):  \n",
    "        slice_metrics = precision_recall_fscore_support(\n",
    "            y_test[mask], y_pred[mask], average=\"micro\"\n",
    "        )\n",
    "        metrics[\"slices\"][slice_name] = {}\n",
    "        metrics[\"slices\"][slice_name][\"precision\"] = slice_metrics[0]\n",
    "        metrics[\"slices\"][slice_name][\"recall\"] = slice_metrics[1]\n",
    "        metrics[\"slices\"][slice_name][\"f1\"] = slice_metrics[2]\n",
    "        metrics[\"slices\"][slice_name][\"num_samples\"] = len(y_test[mask])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(metrics[\"slices\"], indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lime\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier_fn(texts):\n",
    "    ds = ray.data.from_items([{\"title\": text, \"description\": \"\", \"tag\": \"other\"} for text in texts])\n",
    "    preprocessed_ds = preprocessor.transform(ds)\n",
    "    outputs = preprocessed_ds.map_batches(predictor.predict_proba)\n",
    "    y_prob = np.array([d[\"output\"] for d in outputs.take_all()])\n",
    "    return y_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"Using pretrained convolutional neural networks for object detection.\"\n",
    "explainer = LimeTextExplainer(class_names=list(class_to_index.keys()))\n",
    "explainer.explain_instance(text, classifier_fn=classifier_fn, top_labels=1).show_in_notebook(text=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [\"revolutionized\", \"disrupted\"]\n",
    "texts = [f\"Transformers applied to NLP have {token} the ML field.\" for token in tokens]\n",
    "[preprocessor.index_to_class[y_prob.argmax()] for y_prob in classifier_fn(texts=texts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [\"text classification\", \"image classification\"]\n",
    "texts = [f\"ML applied to {token}.\" for token in tokens]\n",
    "[preprocessor.index_to_class[y_prob.argmax()] for y_prob in classifier_fn(texts=texts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [\"natural language processing\", \"mlops\"]\n",
    "texts = [f\"{token} is the next big wave in machine learning.\" for token in tokens]\n",
    "[preprocessor.index_to_class[y_prob.argmax()] for y_prob in classifier_fn(texts=texts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Serving model\n",
    "import ray.data\n",
    "from ray.data import ActorPoolStrategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_id = sorted_runs.iloc[0].run_id\n",
    "best_checkpoint = get_best_checkpoint(run_id=run_id)\n",
    "predictor = TorchPredictor.from_checkpoint(best_checkpoint)\n",
    "preprocessor = predictor.get_preprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_ds = preprocessor.transform(test_ds)\n",
    "compute = ActorPoolStrategy(min_size=1, max_size=2)\n",
    "outputs = preprocessed_ds.map_batches(predictor, batch_size=128, compute=compute)\n",
    "np.array([d[\"output\"] for d in outputs.take_all()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI\n",
    "from ray import serve\n",
    "import requests\n",
    "from starlette.requests import Request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = FastAPI(\n",
    "    title=\"Transfer Learning LLM\",\n",
    "    description=\"Classify machine learning projects.\", \n",
    "    version=\"0.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment(num_replicas=\"1\", ray_actor_options={\"num_cpus\": 8, \"num_gpus\": 0})\n",
    "@serve.ingress(app)\n",
    "class ModelDeployment:\n",
    "    def __init__(self, run_id):\n",
    "        self.run_id = run_id\n",
    "        mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)  \n",
    "        best_checkpoint = get_best_checkpoint(run_id=run_id)\n",
    "        self.predictor = TorchPredictor.from_checkpoint(best_checkpoint)\n",
    "\n",
    "    @app.post(\"/predict/\")\n",
    "    async def _predict(self, request: Request):\n",
    "        data = await request.json()\n",
    "        sample_ds = ray.data.from_items([{\"title\": data.get(\"title\", \"\"), \"description\": data.get(\"description\", \"\"), \"tag\": \"\"}])\n",
    "        results = predict_proba(ds=sample_ds, predictor=self.predictor)\n",
    "        return {\"results\": results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_runs = mlflow.search_runs(experiment_names=[experiment_name], order_by=[\"metrics.val_loss ASC\"])\n",
    "run_id = sorted_runs.iloc[0].run_id\n",
    "serve.run(ModelDeployment.bind(run_id=run_id), route_prefix=\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Transfer learning with transformers\"\n",
    "description = \"Using transformers for transfer learning on text classification tasks.\"\n",
    "json_data = json.dumps({\"title\": title, \"description\": description})\n",
    "requests.post(\"http://127.0.0.1:8000/predict/\", data=json_data).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"this is random noise\"  \n",
    "json_data = json.dumps({\"title\": title, \"description\": \"\"})\n",
    "requests.post(\"http://127.0.0.1:8000/predict/\", data=json_data).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serve.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@serve.deployment(num_replicas=\"1\", ray_actor_options={\"num_cpus\": 8, \"num_gpus\": 0})\n",
    "@serve.ingress(app)\n",
    "class ModelDeploymentRobust:\n",
    "    def __init__(self, run_id, threshold=0.9):\n",
    "        self.run_id = run_id\n",
    "        self.threshold = threshold\n",
    "        mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)  \n",
    "        best_checkpoint = get_best_checkpoint(run_id=run_id)\n",
    "        self.predictor = TorchPredictor.from_checkpoint(best_checkpoint)\n",
    "\n",
    "    @app.post(\"/predict/\")\n",
    "    async def _predict(self, request: Request):\n",
    "        data = await request.json()\n",
    "        sample_ds = ray.data.from_items([{\"title\": data.get(\"title\", \"\"), \"description\": data.get(\"description\", \"\"), \"tag\": \"\"}])\n",
    "        results = predict_proba(ds=sample_ds, predictor=self.predictor)\n",
    "        \n",
    "        for i, result in enumerate(results):\n",
    "            pred = result[\"prediction\"]\n",
    "            prob = result[\"probabilities\"]\n",
    "            if prob[pred] < self.threshold:\n",
    "                results[i][\"prediction\"] = \"other\"\n",
    "\n",
    "        return {\"results\": results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serve.run(ModelDeploymentRobust.bind(run_id=run_id, threshold=0.9), route_prefix=\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"this is random noise\"  \n",
    "json_data = json.dumps({\"title\": title, \"description\": \"\"})\n",
    "requests.post(\"http://127.0.0.1:8000/predict/\", data=json_data).json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "serve.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
